task_name="large_mmi_en"

# k8s settings
mpi_on_k8s=1
fs_name="hdfs://yq01-heng-hdfs.dmop.baidu.com:54310"
fs_ugi="nlp-dialogue,nlp-dialogue"
force_reuse_output_path="True"
output_path="/app/ssg/nlp-dialogue/hehuang/outputs/large_mmi_en"

# job settings
JOB_SCRIPT="./script/run_pretrain.sh"
queue="nlp-16g-0-yq01-k8s-gpu-v100-8"
nodes=4

# data & model & vocab settings
hdfs_path="hdfs://yq01-heng-hdfs.dmop.baidu.com:54310"
hdfs_ugi="nlp-dialogue,nlp-dialogue"
hdfs_output="/app/ssg/nlp-dialogue/hehuang/outputs/$task_name"
train_files_dir="/app/ssg/nlp/ol/hehuang/data/train_v3_multi_user_dialog_for_mmi/"
valid_files_dir="/app/ssg/nlp/ol/hehuang/data/valid_v3_multi_user_dialog_for_mmi/"
train_filelist="./package/dialog_en/train_filelist_for_mmi"
valid_filelist="./package/dialog_en/valid_filelist_for_mmi"
hack_old_data="False"
vocab_path="./package/dialog_en/vocab.txt"
CONFIG_PATH="./package/dialog_en/large_config.json"
hdfs_init_model="/app/ssg/nlp-dialogue/hehuang/outputs/ernie_gen_dialog_pretrain_en/output/stage4_fix_job.step_220000.tar"
init_params="step_220000"

# training settings
in_tokens="True"
lr_scheduler="linear_warmup_decay"
use_fp16="True"
is_unidirectional="True"
loss_scaling=12800
generate_neg_sample="False"

num_train_steps=1000000
validation_steps=20000
SKIP_STEPS=500
SAVE_STEPS=20000
WARMUP_STEPS=4000
BATCH_SIZE=4096
LR_RATE=1e-5
WEIGHT_DECAY=0.01
MAX_LEN=512
EPOCH=10
